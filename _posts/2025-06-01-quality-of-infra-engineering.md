---
title: A Platform for Improving Quality of Infrastructure Engineering in Open Science (I)
layout: post
description: A story about eliminating toil - Part 1
category: platform-engineering
toc: true
tags:
  - blog
  - APIs
  - DevOps
  - SRE
mermaid: true
---

_This is the writeup of a presentation given to EGI 2025 Conference in 2025._

EGI has been in the business of managing federated infrastructure and delivering services for [more than 20 years now](https://www.egi.eu/20-years-with-egi/).
During this time, it has participated in countless infrastructure projects, and brought even more services to the fingertips of user communities across Europe and beyond.
One of the key successes to the long-term stability and maturity of these services is the FitSM standard, which provides requirements and guidelines on managing services in federated environments.
However, EGI has inherited infrastructure and resources from its federation partners over the long years of its existence, and has continued to maintain this legacy platform without explicit evolution of the methods and practices involved in actually building them.

While FitSM is appropriate and useful for governing and managing services at a mature stage of development, it does not provide the opinions, guidelines or patterns for quickly ramping up and iterating the development of infrastructure components themselves, or services deployed in them.

This _has_ been addressed however in environments where fast iteration towards high-quality services is a competitive advantage, one need only refer to the practice of DevOps to have a good frame of reference.
A shorthand introduction to this article might then be:

> The key for making change fast is DevOps,  while the key for managing mature services is FitSM

## Lack of constraints leads to lack of function

So the collaboration, observation and fast feedback patterns of DevOps allow us to more quickly reach a position where governance of a system becomes feasible, and at that point we can start to get value out of the FitSM standard with its clear requirements.
In our context, software systems are built and delivered by federations - independent teams under separate administrative and operational domains.
There there are so many tools available in every phase of the software  development lifecycle that it is almost certain that independent teams have evolve independent toolkits.
This creates a situation where the freedom to select tools to locally optimise the DevOps lifecycle ends up frustrating the effort to globally optimise the FitSM governance.
Since each team is delivering with a different toolkit, implementing machine-readable procedures becomes prohibitively complex.
The only tool really shared by all teams is human language.
The FitSM requirements for the system are then implemented by adopting this **lowest common denominator**, which is human-readable processes, procedures and policies[^revenge_of_llms].
Furthermore, procedures tend to be either intentionally vague, given the opacity across administrative boundaries.
Teams in one domain do not have visibility into the workings of another, and thus need to describe procedures in terms of generic interfaces, rather than specific actions.
While this does have its benefits -- abstract interfaces allow for tooling to change behind the scenes without changing the procedures, for example -- there are better ways to achieve this, providing better understanding of the system.

## Patterns impose constraints

This problem of complexity has been encountered over and over again in enterprise large and small, it is a consistent hurdle which is inevitable once a certain scale is reached.
The boundary between these distinct administrative domains is in fact an analogue of the boundary that has traditionally existed between the development and operations concerns.
Just because we realised that better collaboration between dev and ops could lead to better outcomes overall, there were plenty of ways to do it "wrong".
The frustration generated by the lack of return on DevOps adoption led some in the industry to consider what _patterns_ or indeed _antipatterns_ were present.
This approach led to the publication of "Team Topologies"[^TT], a guide to identifying which interaction model is right for a given environment.
Emerging from this research was the concept of a "platform" -- an abstract set of functions which, when organised together, allow the teams involved in software delivery to perform their tasks with less cognitive load, fewer dependencies and faster feedback.
A few iterations later, and we arrive at a semi-codification of this platform pattern: **platform engineering**.
"Platform engineering" is the practice of building platforms[^mf_platforms] _design for delivery_.
Another way of thinking of this is that a platform is "the product which helps us deliver services to our customers".
This is the link between our fast-flow, fast-feedback, tool-integration DevOps world and our quality-oriented, requirements-first, process-aligned, service management FitSM world.
The platform engineering pattern helps the various administrative domains adopt a consistent pattern, even if there cannot be overall consensus in the specific tooling.
It organises tools according to their function and position in the software delivery lifecycle, identifying certain "planes":

1. **Developer Control Plane**: contains all the necessary tooling for developers to develop their applications.
This includes version control for the applications themselves, as well as the source code for the infrastructure of the platform itself.
In more mature cases, includes an internal service catalogue and API gateway which developers can use to identify re-usable components and avoid unnecessary duplication.
1. **Integration and Delivery Plane**: Work performed in the Developer Control Plane then flows to the Integration and Delivery Plane.
This plane includes continuous integration (CI) pipelines, artifactories, and continuous deployment (CD) pipelines.
A crucial component of the Integration and Delivery Plane is the platform orchestrator, which, given platform definitions, configuration constraints, policies, _etc_, deploys new artifacts into their deployment environments.
1. **Resource Plane**: The Resource Plane contains the actual operating environment of the services offered to customers.
This is the traditional "Ops" part of DevOps, which is bound to reliability, security, cost, _etc_ objectives.
The resource plane includes all environments, including the testing, staging and production environments.
1. **Observability Plane**: The Observability Plane monitors all components in the platform itself, including the Resource Plane, and is the bedrock for feedback.
The Observability Plane collects metrics from all of the platform components themselves, as well as the services deployed into the Resource Plane, and alerts based on defined service level objectives.
Furthermore, it aggregates the logs and traces from across the platform.
1. **Security Plane**: The Security Plane is responsible for declaring, enforcing and supporting security and safety across the platform and the workloads which it serves.

<figure>
    <img src="{{ site_url }}/images/platform-plane-overview.png" />
</figure>

We are still hiding the complexities of the actual tooling behind the abstraction of the "plane", but at least we now have a common model to work with across administrative domains.

## Overcoming boundaries

The platform engineering pattern at least tames the complexity of the DevOps space, but we are still left with the organisational boundaries inherent in federations.
Have we improved the situation by adopting the idea of a platform, or have we just made more work for ourselves, since now every team has to build a platform?
Organisational boundaries will never simply go away, and even though our platform describes patterns of interaction, we still need to actually _implement_ those interactions.
Code commits still need to flow through CI, workload definitions still need to audited, secured and delivered to various resource planes across organisational boundaries (_ie_ heterogeneous toolkits)
The question becomes:

> How can we build a platform while still respecting the need for organisational boundaries?

Given that components of the platform, or even entire planes of the platform may be provided by a different organisation, the question becomes:

> How can I consume the services of a platform component, or expose the services of my component, across opaque organisational boundaries?

This may be an analagous to the "Bezos API Mandate"[^BezosAPIMandate].
To paraphrase the paraphrased, it said[^edited]:

1. All teams will henceforth expose their data and functionality through service interfaces.
1. Teams must communicate with each other through these interfaces.
1. There will be no other form of interprocess communication allowed
1. It doesn't matter what technology they use
1. All service interfaces, ... must be designed ... to be externalizable.

Wouldn't that be great, if all of the functionality in our platform that we needed to deliver our service to users were exposed as an API?

## Workflows in platforms vs tools

Now, most of the tools in the toolkit do indeed expose APIs which people or programs can consume externally.
The problem is that we have very deliberately made reference to explicit tooling in our platform pattern.
When the platform is eventually created, it takes the form of actual tools and if the only way to use those tools in our delivery workflows is via their specific APIs, we are back to the state of complexity we tried to address in the first place.
We would have to integrate with any number of APIs, take account of any number of specific configurations, in order to make use of the platform.
This is clearly not an improvement on the situation!

## Event-driven architectures

Tightly-integrated end-to-end workflows for change management, release, deployment, monitoring, _etc_ -- _i.e._ all of the things that need to be done in a service management system -- are thus doomed.
Either they are too small in scope (only a single org, or team), or are too costly to implement (too many different tools and APIs) or maintain (tools come and go, so we need to keep working on the integrations with other APIs).
But, what if we didn't have to create end-to-end workflows?
An alternative approach to implementing procedures would be to take an event-driven view of the system.
That would allow us to write procedures as the combination of specific events, actors and actions:

When: _event_
Then: _action_
By: _actor_

The action can then be codified, while policy defined in the SMS defines who or what the actor is.
What remains is to decide on the triggering _event_, and determine the means by which the actor is notified of that event.

This is where the [CDEvents specification](https://cdevents.dev) comes in.
As we will see in the next post, the common specification of events in a software system will allow us to build a tool-agnostic platform for delivery of software systems to our customers, across organisational boundaries.

This will be discussed in Part II, where we dive into the CDEvents specification, and show how it can solve our complexity and integration problems.


---
# Footnotes and References
[^revenge_of_llms]: Perhaps one day large language models (LLMs) _will_ make human language will a valid interface for implementing machine-readable workflows, but we are most definitely not there yet.
[^TT]: Skelton, M., & Pais, M. (2019). Team topologies : organizing business and technology teams for fast flow. It Revolution.
[^mf_platforms]: The term "platform" will arise repeatedly in this article. To be clear what we mean, we are referring to the same platform that Evan Bottcher refers to in his article [What I talk about when I talk about platforms](https://martinfowler.com/articles/talk-about-platforms.html)
[^BezosAPIMandate]: The only attribution for this I could find was the second-hand, accidental [post by Steve Yegge](https://web.archive.org/web/20151209104319/https://plus.google.com/+RipRowan/posts/eVeouesvaVX). It has been almost lost to time thanks to the shutdown of Google+ (irony of ironies).
[^edited]: This is not faithfully reproduced, I have edited and omitted several parts. See the original for the greater context.
